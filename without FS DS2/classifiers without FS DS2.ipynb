{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "clas='NB'\n",
    "\n",
    "# from sklearn import svm\n",
    "# classifier = svm.SVC(kernel='linear')\n",
    "# clas='SVM linear'\n",
    "\n",
    "# from sklearn import svm\n",
    "# classifier = svm.SVC(kernel='rbf')\n",
    "# clas='SVM RBF'\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "# clas='KNN'\n",
    "\n",
    "# from sklearn import linear_model\n",
    "# classifier = linear_model.LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "# clas='Linear Regression'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepration(file):\n",
    "    data = arff.loadarff(file)\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for i in data[0]:\n",
    "\n",
    "        X.append(list(i)[0:len(i)-2])\n",
    "        if(i[len(i)-1] == b'Y'):\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        \n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder=\"D:\\\\SDP\\\\Afnan\"\n",
    "files = [os.path.join(root_folder, x) for x in os.listdir(root_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC(y_true,y_pred):\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from scipy import interp\n",
    "\n",
    "    from sklearn.metrics import roc_curve  \n",
    "    from sklearn.metrics import roc_auc_score ,auc\n",
    "    from  sklearn.preprocessing import label_binarize \n",
    "    \n",
    "    classes=list(set(y))\n",
    "    y_test = label_binarize(y_true, classes=classes)\n",
    "    y_score = label_binarize(y_pred, classes=classes)\n",
    "\n",
    "    n_classes = len(classes)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:,i], y_score[:,i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    #auc = roc_auc_score(y,y_pred,average='micro')  \n",
    "      \n",
    "    \n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    \n",
    "    return roc_auc,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM1 dataset...\n",
      "subset performance= 0.7685185185185185\n",
      "JM1 dataset...\n",
      "subset performance= 0.7917477617750097\n",
      "KC1 dataset...\n",
      "subset performance= 0.731457800511509\n",
      "KC3 dataset...\n",
      "subset performance= 0.7384615384615385\n",
      "MC1 dataset...\n",
      "subset performance= 0.9604261796042618\n",
      "MW1 dataset...\n",
      "subset performance= 0.75\n",
      "PC1 dataset...\n",
      "subset performance= 0.8798283261802575\n",
      "PC2 dataset...\n",
      "subset performance= 0.45121951219512196\n",
      "PC3 dataset...\n",
      "subset performance= 0.3595505617977528\n",
      "PC4 dataset...\n",
      "subset performance= 0.8705882352941177\n",
      "PC5 dataset...\n",
      "subset performance= 0.7221238938053097\n"
     ]
    }
   ],
   "source": [
    "fnum=-1\n",
    "filexs=pd.DataFrame(columns=[\"Dataset\", \"precision\", \"Accuracy\",\"Recall\",\"F-measure\",\"AUC\",\"G-mean\"])\n",
    "for file in files :\n",
    "    fnum+=1\n",
    "    df=pd.read_csv(file)\n",
    "    if( 'version' in df.columns):\n",
    "        X=np.array(df.drop(['bug','name.1','name', 'version'], axis=1))\n",
    "    else:\n",
    "        X=np.array(df.drop('bug', axis=1))\n",
    "    y=np.array(df.bug)\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    if(clas=='NB'):\n",
    "        classifier = GaussianNB()\n",
    "    elif(clas=='SVM linear'):\n",
    "        classifier = svm.SVC(kernel='linear')\n",
    "    elif(clas=='KNN'):\n",
    "        classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    elif(clas=='SVM RBF'):\n",
    "        classifier = svm.SVC(kernel='rbf')\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred= classifier.predict(X_test)  \n",
    "    \n",
    "    \n",
    "    print(len(list(set(y))))\n",
    "    if(len(list(set(y)))>2):\n",
    "        print('multi')\n",
    "        matrix = multilabel_confusion_matrix(y_test, y_pred)\n",
    "        tn = matrix[:, 0, 0]\n",
    "        tp = matrix[:, 1, 1]\n",
    "        fn = matrix[:, 1, 0]\n",
    "        fp = matrix[:, 0, 1]\n",
    "        G_mean= math.sqrt((tp.mean()/(tp.mean()+fn.mean()))*(tn.mean()/(fp.mean()+tn.mean())))\n",
    "    else:\n",
    "        print('binary')\n",
    "        matrix= confusion_matrix(y_test,y_pred, labels=list(set(y)))\n",
    "        tn, fp, fn, tp = matrix.ravel()\n",
    "        G_mean= math.sqrt((tp/(tp+fn))*(tn/(fp+tn)))\n",
    "    subset_performance = (y_pred== y_test).mean()\n",
    "    ac=accuracy_score(y_test,y_pred)\n",
    "    prec=precision_score(y_test,y_pred,average='weighted')\n",
    "    rc=recall_score(y_test,y_pred,average='weighted')\n",
    "    F_Score = (2 * rc * prec) / (rc + prec)  \n",
    "    auc=AUC(y_test,y_pred)[0][\"micro\"]\n",
    "    \n",
    "    \n",
    "    report=classification_report(y_test,y_pred)\n",
    "    subset_performance = (y_pred== y_test).mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds=file.split(\"\\\\\")[4][0:-4]\n",
    "    print(\"{} dataset...\".format(ds))\n",
    "    \n",
    "    \n",
    "    fr = open(\"Test/without FS/{}/{}.txt\".format(clas,ds), \"a\")\n",
    "   \n",
    "    \n",
    "    \n",
    "    print(\"subset performance= {}\".format(subset_performance))\n",
    "    fr.write(\"\\nclassification_report:\\n\"+str(report)+\"\\n\")\n",
    "    fr.write(\"subset performance = {}\".format(subset_performance))\n",
    "    \n",
    "     fr.write(\"accuracy_score= {}\\n\".format(ac))\n",
    "    fr.write(\"precision_score= {}\\n\".format(prec))\n",
    "    fr.write(\"recall_score= {}\\n\".format(rc))\n",
    "    fr.write(\"F_Score= {}\\n\".format(F_Score))\n",
    "    fr.write(\"auc= {}\\n\".format(auc))\n",
    "    fr.write(\"G_mean= {}\\n\".format(G_mean))\n",
    "    \n",
    "    \n",
    "    \n",
    "    fr.write(\"confusion_matrix=\\n{}\\n\".format(matrix))\n",
    "    fr.write(\"labels {}\\n\".format(set(y)))\n",
    "    fr.write(\"True Negative= {}\\n\".format(tn))\n",
    "    fr.write(\"True Positive= {}\\n\".format(tp))\n",
    "    fr.write(\"Flase Negative= {}\\n\".format(fn))\n",
    "    fr.write(\"False Positive= {}\\n\".format(fp))\n",
    "    fr.close()\n",
    "    \n",
    "    filexs.loc[fnum] =[ds,prec,ac,rc,F_Score,auc,G_mean]\n",
    "filexs.to_excel(\"withoutFS DS2 {}.xlsx\".format(clas))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
